{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fcccd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 00 - Setup Spark + Parquet (verificacoes rapidas)\n",
    "# Execute cada celula na ordem. Este notebook nao processa dados; so garante o ambiente pronto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce638c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Garantir acesso aos modulos do projeto (../)\n",
    "import sys\n",
    "from pathlib import Path\n",
    "PROJECT_ROOT = Path.cwd().resolve().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "print(f\"PROJECT_ROOT adicionado ao sys.path: {PROJECT_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33b94f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.6 | packaged by conda-forge | (main, Oct  3 2023, 10:40:35) [GCC 12.3.0]\n",
      "Working dir: /home/jovyan/work/notebooks\n",
      "Env PATH has Java? False\n",
      "openjdk version \"17.0.8.1\" 2023-08-24\n",
      "OpenJDK Runtime Environment (build 17.0.8.1+1-Ubuntu-0ubuntu122.04)\n",
      "OpenJDK 64-Bit Server VM (build 17.0.8.1+1-Ubuntu-0ubuntu122.04, mixed mode, sharing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys, os, shutil, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Working dir: {Path.cwd()}\")\n",
    "print(f\"Env PATH has Java? {'java' in os.environ.get('PATH','').lower()}\")\n",
    "\n",
    "# Checar Java instalado\n",
    "java = shutil.which('java')\n",
    "if java:\n",
    "    try:\n",
    "        out = subprocess.check_output([java, '-version'], stderr=subprocess.STDOUT, text=True)\n",
    "        print(out)\n",
    "    except Exception as exc:  # noqa: BLE001\n",
    "        print(f\"Aviso: falha ao ler java -version: {exc}\")\n",
    "else:\n",
    "    print('Java nao encontrado no PATH. Instale Java 8+ e reabra o terminal.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8120e739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle.json criado em /home/jovyan/.kaggle/kaggle.json\n"
     ]
    }
   ],
   "source": [
    "# Definir credenciais do Kaggle aqui (opcional). Preencha e execute para criar ~/.kaggle/kaggle.json.\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "KAGGLE_USERNAME = 'marcowandraski'  # coloque aqui o username\n",
    "KAGGLE_KEY = '2c5901cf78547b038c2e88f0828128e1'       # coloque aqui a key\n",
    "\n",
    "if KAGGLE_USERNAME and KAGGLE_KEY:\n",
    "    kaggle_dir = Path.home() / '.kaggle'\n",
    "    kaggle_dir.mkdir(parents=True, exist_ok=True)\n",
    "    cred_path = kaggle_dir / 'kaggle.json'\n",
    "    cred_path.write_text(json.dumps({'username': KAGGLE_USERNAME, 'key': KAGGLE_KEY}))\n",
    "    try:\n",
    "        cred_path.chmod(0o600)\n",
    "    except Exception:\n",
    "        pass\n",
    "    print(f'kaggle.json criado em {cred_path}')\n",
    "else:\n",
    "    print('KAGGLE_USERNAME/KAGGLE_KEY vazios. Preencha e reexecute esta celula se precisar baixar dados do Kaggle.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "766850f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aviso: pip install falhou (talvez ja instaladas?): Command '['/opt/conda/bin/python', '-m', 'pip', 'install', '-r', 'requirements.txt']' returned non-zero exit status 1.\n"
     ]
    }
   ],
   "source": [
    "# Instalar dependencias (se ainda nao instalou no ambiente)\n",
    "# Comente se ja fez: pip install -r requirements.txt\n",
    "import sys, subprocess\n",
    "\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-r', 'requirements.txt'])\n",
    "    print('Dependencias instaladas/atualizadas.')\n",
    "except Exception as exc:  # noqa: BLE001\n",
    "    print(f'Aviso: pip install falhou (talvez ja instaladas?): {exc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2799e5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credenciais Kaggle encontradas em /home/jovyan/.kaggle/kaggle.json\n"
     ]
    }
   ],
   "source": [
    "# Verificar kaggle credenciais (necessario para notebooks Bronze)\n",
    "from pathlib import Path\n",
    "kaggle_json = Path.home() / '.kaggle' / 'kaggle.json'\n",
    "if kaggle_json.exists():\n",
    "    print(f'Credenciais Kaggle encontradas em {kaggle_json}')\n",
    "else:\n",
    "    print('Credenciais Kaggle NAO encontradas. Adicione kaggle.json em ~/.kaggle ou preencha a celula de credenciais acima.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18e49866",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spark_jobs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Criar diretorios de dados\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspark_jobs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ensure_data_dirs, BRONZE_PATH, SILVER_PATH, GOLD_DIR\n\u001b[1;32m      3\u001b[0m ensure_data_dirs()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDiretorios criados/verificados:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spark_jobs'"
     ]
    }
   ],
   "source": [
    "# Criar diretorios de dados\n",
    "from spark_jobs.config import ensure_data_dirs, BRONZE_PATH, SILVER_PATH, GOLD_DIR\n",
    "ensure_data_dirs()\n",
    "print('Diretorios criados/verificados:')\n",
    "print(f' - Bronze: {BRONZE_PATH}')\n",
    "print(f' - Silver: {SILVER_PATH}')\n",
    "print(f' - Gold dir: {GOLD_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "233be624",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spark_jobs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Testar criacao de SparkSession e escrita/ leitura de parquet temporario\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspark_jobs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspark_session_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_spark_session\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functions \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Row\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spark_jobs'"
     ]
    }
   ],
   "source": [
    "# Testar criacao de SparkSession e escrita/ leitura de parquet temporario\n",
    "from spark_jobs.spark_session_manager import get_spark_session\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "from pathlib import Path\n",
    "\n",
    "spark = get_spark_session('Smoke Test')\n",
    "\n",
    "tmp_path = Path('data/_tmp_spark_check.parquet')\n",
    "\n",
    "# Dataset pequeno para validar escrita/leitura\n",
    "sample = [Row(id=i, value=f'v{i}') for i in range(5)]\n",
    "df = spark.createDataFrame(sample)\n",
    "df = df.withColumn('ts', F.current_timestamp())\n",
    "\n",
    "print('Escrevendo parquet temporario...')\n",
    "df.write.mode('overwrite').parquet(str(tmp_path))\n",
    "print(f'Parquet escrito em {tmp_path}')\n",
    "\n",
    "print('Lendo parquet temporario...')\n",
    "df2 = spark.read.parquet(str(tmp_path))\n",
    "df2.show(truncate=False)\n",
    "\n",
    "print('OK: Spark leu/escreveu Parquet.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f10fbf6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:  \u001b[38;5;66;03m# noqa: BLE001\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAviso: nao foi possivel remover tmp parquet: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSetup concluido.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Limpar arquivo temporario (opcional)\n",
    "try:\n",
    "    tmp_path = Path('data/_tmp_spark_check.parquet')\n",
    "    if tmp_path.exists():\n",
    "        import shutil\n",
    "        shutil.rmtree(tmp_path)\n",
    "        print(f'Removido {tmp_path}')\n",
    "except Exception as exc:  # noqa: BLE001\n",
    "    print(f'Aviso: nao foi possivel remover tmp parquet: {exc}')\n",
    "\n",
    "spark.stop()\n",
    "print('Setup concluido.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
