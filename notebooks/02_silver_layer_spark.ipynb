{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff7c677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "343032a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    DecimalType,\n",
    "    DoubleType,\n",
    "    FloatType,\n",
    "    IntegerType,\n",
    "    LongType,\n",
    "    ShortType,\n",
    "    StringType,\n",
    ")\n",
    "from pyspark.ml.feature import Imputer\n",
    "from spark_jobs.config import BRONZE_PATH, SILVER_PATH, ensure_data_dirs\n",
    "from spark_jobs.spark_session_manager import get_spark_session\n",
    "\n",
    "\n",
    "def normalize_columns(df):\n",
    "    current = df\n",
    "    for name in df.columns:\n",
    "        new_name = re.sub(r\"\\s+\", \" \", name).strip().upper()\n",
    "        if new_name != name:\n",
    "            current = current.withColumnRenamed(name, new_name)\n",
    "    return current\n",
    "\n",
    "\n",
    "def drop_null_keys(df):\n",
    "    missing = [c for c in (\"STATE\", \"DISTRICT\") if c not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"Warning: columns missing for null check: {missing}\")\n",
    "        return df\n",
    "    return df.dropna(subset=[\"STATE\", \"DISTRICT\"])\n",
    "\n",
    "\n",
    "def impute_numerics(spark, df):\n",
    "    numeric_cols = [\n",
    "        field.name\n",
    "        for field in df.schema.fields\n",
    "        if isinstance(\n",
    "            field.dataType,\n",
    "            (DoubleType, IntegerType, FloatType, LongType, ShortType, DecimalType),\n",
    "        )\n",
    "        and field.name not in [\"YEAR\"]\n",
    "    ]\n",
    "    if not numeric_cols:\n",
    "        print(\"No numeric columns to impute.\")\n",
    "        return df\n",
    "    imputer = Imputer(\n",
    "        inputCols=numeric_cols,\n",
    "        outputCols=[f\"{c}_imputed\" for c in numeric_cols],\n",
    "        strategy=\"median\",\n",
    "    )\n",
    "    model = imputer.fit(df)\n",
    "    df_imputed = model.transform(df)\n",
    "    for c in numeric_cols:\n",
    "        df_imputed = df_imputed.drop(c).withColumnRenamed(f\"{c}_imputed\", c)\n",
    "    print(\"Numeric nulls filled with median.\")\n",
    "    return df_imputed\n",
    "\n",
    "\n",
    "def fill_categoricals(df):\n",
    "    text_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, StringType)]\n",
    "    if not text_cols:\n",
    "        return df\n",
    "    print(\"String nulls filled with 'Unknown'.\")\n",
    "    return df.fillna(\"Unknown\", subset=text_cols)\n",
    "\n",
    "\n",
    "def remove_outliers_iqr(df, cols_to_check):\n",
    "    cols = [c for c in cols_to_check if c in df.columns]\n",
    "    if not cols:\n",
    "        print(\"No numeric columns found for outlier removal.\")\n",
    "        return df\n",
    "    quantiles_data = df.approxQuantile(cols, [0.25, 0.75], 0.05)\n",
    "    filter_condition = F.lit(True)\n",
    "    before = df.count()\n",
    "    for idx, col_name in enumerate(cols):\n",
    "        q1, q3 = quantiles_data[idx]\n",
    "        iqr = q3 - q1\n",
    "        lower = q1 - (1.5 * iqr)\n",
    "        upper = q3 + (1.5 * iqr)\n",
    "        col_filter = (F.col(col_name) >= lower) & (F.col(col_name) <= upper)\n",
    "        filter_condition = filter_condition & col_filter\n",
    "        print(f\"Outlier bounds for {col_name}: [{lower:.2f}, {upper:.2f}]\")\n",
    "    filtered = df.filter(filter_condition)\n",
    "    after = filtered.count()\n",
    "    print(f\"Outliers removed: {before - after}\")\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def standardize_and_filter(df):\n",
    "    current = df\n",
    "    for column_name in (\"STATE\", \"DISTRICT\", \"CROP\", \"SEASON\"):\n",
    "        if column_name in current.columns:\n",
    "            current = current.withColumn(column_name, F.trim(F.upper(F.col(column_name))))\n",
    "    if \"STATE\" in current.columns:\n",
    "        current = current.filter(F.col(\"STATE\") != \"STATE\")\n",
    "    if \"STATE\" in current.columns and \"DISTRICT\" in current.columns:\n",
    "        current = current.withColumn(\n",
    "            \"STATE_DISTRICT\", F.concat(F.col(\"STATE\"), F.lit(\" - \"), F.col(\"DISTRICT\"))\n",
    "        )\n",
    "    desired_crops = [\"MAIZE\", \"RICE\", \"WHEAT\", \"BARLEY\"]\n",
    "    if \"CROP\" in current.columns:\n",
    "        current = current.filter(F.col(\"CROP\").isin(desired_crops))\n",
    "    if \"AREA\" in current.columns:\n",
    "        current = current.filter(F.col(\"AREA\") > 0)\n",
    "    if \"YIELD\" in current.columns:\n",
    "        current = current.filter(F.col(\"YIELD\") > 0)\n",
    "    print(f\"Records after standardization and filters: {current.count()}\")\n",
    "    return current\n",
    "\n",
    "\n",
    "def rename_to_snake_case(df):\n",
    "    def _snake(name: str) -> str:\n",
    "        name = re.sub(r\"[\\s\\-\\/\\.]+\", \"_\", name.strip())\n",
    "        name = re.sub(r\"[^A-Za-z0-9_]\", \"\", name)\n",
    "        name = re.sub(r\"_+\", \"_\", name)\n",
    "        return name.lower().strip(\"_\")\n",
    "    current = df\n",
    "    for name in df.columns:\n",
    "        new_name = _snake(name)\n",
    "        if new_name != name:\n",
    "            current = current.withColumnRenamed(name, new_name)\n",
    "    return current\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9836b891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- District : string (nullable = true)\n",
      " |-- Crop: string (nullable = true)\n",
      " |-- Season: string (nullable = true)\n",
      " |-- Area: double (nullable = true)\n",
      " |-- Production: integer (nullable = true)\n",
      " |-- Yield: double (nullable = true)\n",
      " |-- JAN: double (nullable = true)\n",
      " |-- FEB: double (nullable = true)\n",
      " |-- MAR: double (nullable = true)\n",
      " |-- APR: double (nullable = true)\n",
      " |-- MAY: double (nullable = true)\n",
      " |-- JUN: double (nullable = true)\n",
      " |-- JUL: double (nullable = true)\n",
      " |-- AUG: double (nullable = true)\n",
      " |-- SEP: double (nullable = true)\n",
      " |-- OCT: double (nullable = true)\n",
      " |-- NOV: double (nullable = true)\n",
      " |-- DEC: double (nullable = true)\n",
      " |-- ANN: double (nullable = true)\n",
      " |-- Jan-Feb: double (nullable = true)\n",
      " |-- Mar-May: double (nullable = true)\n",
      " |-- Jun-Sep: double (nullable = true)\n",
      " |-- Oct-Dec: double (nullable = true)\n",
      " |-- TEMP_ANNUAL: double (nullable = true)\n",
      " |-- TEMP_JAN_FEB: double (nullable = true)\n",
      " |-- TEMP_MAR_MAY: double (nullable = true)\n",
      " |-- TEMP_JUN_SEP: double (nullable = true)\n",
      " |-- TEMP_OCT_DEC: double (nullable = true)\n",
      " |-- data_ingestao: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ensure_data_dirs()\n",
    "spark = get_spark_session('Silver Layer Notebook')\n",
    "\n",
    "bronze_path = Path(BRONZE_PATH)\n",
    "if not bronze_path.exists():\n",
    "    raise FileNotFoundError(f'Bronze parquet not found at {bronze_path}')\n",
    "\n",
    "df_bronze = spark.read.parquet(str(bronze_path))\n",
    "df_bronze.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39402f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric nulls filled with median.\n",
      "String nulls filled with 'Unknown'.\n",
      "Outlier bounds for PRODUCTION: [-8750.00, 14850.00]\n",
      "Outlier bounds for AREA: [-5169.50, 8834.50]\n",
      "Outlier bounds for YIELD: [-2.02, 4.75]\n",
      "Outliers removed: 106951\n",
      "Records after standardization and filters: 28908\n"
     ]
    }
   ],
   "source": [
    "df_norm = normalize_columns(df_bronze)\n",
    "df_keys = drop_null_keys(df_norm)\n",
    "df_imputed = impute_numerics(spark, df_keys)\n",
    "df_filled = fill_categoricals(df_imputed)\n",
    "\n",
    "outlier_columns = [c for c in ['PRODUCTION', 'AREA', 'YIELD'] if c in df_filled.columns]\n",
    "df_no_outliers = remove_outliers_iqr(df_filled, outlier_columns)\n",
    "df_standard = standardize_and_filter(df_no_outliers)\n",
    "\n",
    "df_silver = rename_to_snake_case(df_standard).withColumn(\n",
    "    'data_processamento', F.current_timestamp()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2feacb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver saved to /home/jovyan/work/data/silver/dados_limpos.parquet\n",
      "+--------------------+-----+----+-----+\n",
      "|      state_district| crop|year|yield|\n",
      "+--------------------+-----+----+-----+\n",
      "|ANDAMAN AND NICOB...|MAIZE|2009| 2.48|\n",
      "|ANDAMAN AND NICOB...| RICE|2003| 1.73|\n",
      "|ANDAMAN AND NICOB...| RICE|2004| 1.37|\n",
      "|ANDHRA PRADESH - ...|MAIZE|2004|  2.2|\n",
      "|ANDHRA PRADESH - ...|MAIZE|1999| 2.84|\n",
      "|ANDHRA PRADESH - ...|MAIZE|2003| 1.72|\n",
      "|ANDHRA PRADESH - ...|MAIZE|2004| 2.87|\n",
      "|ANDHRA PRADESH - ...|MAIZE|1999| 2.84|\n",
      "|ANDHRA PRADESH - ...|MAIZE|2003| 1.72|\n",
      "|ANDHRA PRADESH - ...|MAIZE|2004| 2.87|\n",
      "+--------------------+-----+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_silver.write.mode('overwrite').parquet(str(SILVER_PATH))\n",
    "print(f'Silver saved to {SILVER_PATH}')\n",
    "df_silver.select('state_district', 'crop', 'year', 'yield').show(10)\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
